{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Mode 0 referes to a 10W mode and mode 1 refers to a 5W mode. You could load all the modules and instantiations in the 10W mode \n",
    "# which makes it faster and do only the Inference in the 5W mode. \n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "password = getpass.getpass()\n",
    "command = \"sudo -S nvpmodel -m 0\" \n",
    "os.popen(command, 'w').write(password + '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Use this only if you have to reprogram the arduino. This will prorgam the .ino file found in the 'arduino_sketchbook' folder\n",
    "# to the attached arduino via the USB UART.\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "%cd arduino_sketchbook_following_car\n",
    "! make clean upload;\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Loading necessary libraries and their instantiations. Make sure that the Camera instantation happens only once or it ends up \n",
    "# breaking the notebook and would require a restart.\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging, sys\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "from PIL import Image, ImageDraw\n",
    "import tensorflow.contrib.tensorrt as trt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import PIL.Image as pil\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import torch\n",
    "# import depth_perception_files.networks as dp_networks\n",
    "# from torchvision import transforms\n",
    "\n",
    "from misc_scripts.sort import *\n",
    "kalman_object_tracker = Sort()\n",
    "\n",
    "import serial\n",
    "ser = serial.Serial('/dev/ttyACM0', 57600, timeout = 0.5)\n",
    "\n",
    "# Make sure you initialize only once.\n",
    "from misc_scripts.rpi_camera import CSICamera\n",
    "camera = CSICamera(width = 300, height = 300)\n",
    "\n",
    "import ipywidgets\n",
    "from ipywidgets import HBox\n",
    "\n",
    "output_image_widget = ipywidgets.Image(format = 'jpeg')\n",
    "optical_flow_vector_image_widget = ipywidgets.Image(format = 'jpeg')\n",
    "depth_perception_widget = ipywidgets.Image(format = 'jpeg')\n",
    "\n",
    "from IPython.display import display\n",
    "import traitlets\n",
    "\n",
    "print('Primary initializations complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Loads the PyTorch modules required for the Depth Perception. Advisable to comment this and anything related to depth perception \n",
    "# out when you require only the Platooning system to be active.\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# model_name = \"mono_640x192\"\n",
    "\n",
    "# model_path = os.path.join(\"depth_perception_files/models\", model_name)\n",
    "# encoder_path = os.path.join(\"depth_perception_files/models\", model_name, \"encoder.pth\")\n",
    "# depth_decoder_path = os.path.join(\"depth_perception_files/models\", model_name, \"depth.pth\")\n",
    "\n",
    "# device = torch.device(\"cuda\")\n",
    "\n",
    "# encoder = dp_networks.ResnetEncoder(18, False)\n",
    "# loaded_dict_enc = torch.load(encoder_path, map_location=device)\n",
    "\n",
    "# feed_height = loaded_dict_enc['height']\n",
    "# feed_width = loaded_dict_enc['width']\n",
    "\n",
    "# filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}\n",
    "# encoder.load_state_dict(filtered_dict_enc)\n",
    "# encoder.to(device)\n",
    "# encoder.eval()\n",
    "\n",
    "# depth_decoder = dp_networks.DepthDecoder(num_ch_enc=encoder.num_ch_enc, scales=range(4))\n",
    "\n",
    "# loaded_dict = torch.load(depth_decoder_path, map_location=device)\n",
    "# depth_decoder.load_state_dict(loaded_dict)\n",
    "\n",
    "# depth_decoder.to(device)\n",
    "# depth_decoder.eval()\n",
    "\n",
    "# print('Depth Perception system initializations complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# A trt optimized graph is stored as a .pb file. This function loads it.\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def get_frozen_graph(graph_file):\n",
    "    # Read Frozen Graph file from disk.\n",
    "    with tf.gfile.FastGFile(graph_file, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    return graph_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_graph = get_frozen_graph('object_detection_files/models/object_detection_27_10_19.pb')\n",
    "input_names = ['image_tensor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Create a tensorflow session with a few further optimizations to the trt graph (common sub expression elimination, function \n",
    "# inlining etc.). Also allows dynamic allocation of the GPU memory.\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "graph_options = tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L1))\n",
    "\n",
    "tf_config = tf.ConfigProto(graph_options=graph_options)\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "\n",
    "tf_sess = tf.Session(config=tf_config)\n",
    "\n",
    "tf.import_graph_def(trt_graph, name='')\n",
    "\n",
    "tf_input = tf_sess.graph.get_tensor_by_name(input_names[0] + ':0')\n",
    "tf_scores = tf_sess.graph.get_tensor_by_name('detection_scores:0')\n",
    "tf_boxes = tf_sess.graph.get_tensor_by_name('detection_boxes:0')\n",
    "tf_classes = tf_sess.graph.get_tensor_by_name('detection_classes:0')\n",
    "tf_num_detections = tf_sess.graph.get_tensor_by_name('num_detections:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Zero position values for the throttle, steering, pan and tilt mechanism. 90-92 is the zero throttle position and values > 92 \n",
    "# correspond to reverse (till maybe 110) and values < 92 correspond to forward (till 85-80). \n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "zero_steering = 90\n",
    "zero_throttle = 92\n",
    "zero_pan = 90\n",
    "zero_tilt = 20\n",
    "\n",
    "# Maximum and minimum areas of the bounding box necessary for throttle mapping \n",
    "max_area = 45000\n",
    "min_area = 2000\n",
    "\n",
    "# The safe area (safe distance) to maintain between the two cars\n",
    "central_area = 30000\n",
    "\n",
    "# Maximum and minimum throttle values for forward and reverse\n",
    "min_throttle_forward = 90 # lower limit (slowest)\n",
    "max_throttle_forward = 88 # upper limit (fastest)\n",
    "min_throttle_reverse = 100 # lower limit (slowest)\n",
    "max_throttle_reverse = 102 # upper limit (fastest)\n",
    "\n",
    "# Global declarations \n",
    "crawling_throttle = 89\n",
    "last_known_position = 555 # arbitrary number other than zero that might never occur\n",
    "last_known_pan = 555\n",
    "\n",
    "lost_timer = 0\n",
    "\n",
    "frame1 = camera.value\n",
    "test_frame_1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "prev_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Follows from the arduino 'map' funciton used to translate one range of values to another range. \n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "def constrain(val, min_val, max_val):\n",
    "    return min(max_val, max(min_val, val))\n",
    "\n",
    "def mapper(x, in_min, in_max, out_min, out_max):\n",
    "    return int((x - in_min) * (out_max - out_min) / (in_max - in_min) + out_min)   \n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Converts image to a bytes format for display with a Jupyter Image widget.\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "def bgr8_to_jpeg(value, quality = 75):\n",
    "    return bytes(cv2.imencode('.jpg', value)[1])\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Send Throttle, Steering, Pan and Tilt values for the servos to the arduino connected via the USB UART serial.\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "def serial_writer(steering, throttle, pan, tilt):\n",
    "    steering = format(int(steering), \"03d\")\n",
    "    throttle = format(int(throttle), \"03d\")\n",
    "    pan = format(int(pan), \"03d\")\n",
    "    tilt = format(int(tilt), \"03d\")\n",
    "    val =  steering + ',' + throttle + ',' + pan + ',' + tilt + '*'\n",
    "    ser.write(str(val).encode())\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Function to calculate the distance to the object. This is specific to each kind of object as we depend on actual height of the\n",
    "# object itself. Follows from the formula:\n",
    "#\n",
    "#      Distance to object(mm) = focal_length(mm) * actual object height(mm) * image height (pixels)\n",
    "#                               -------------------------------------------------------------------\n",
    "#                                       object height(pixels) * height of the camera(mm)\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "def distance_to_object(object_height_pixels):    \n",
    "    image_height_pixels = 300  \n",
    "    focal_length_mm = 3.5    \n",
    "    sensor_height_mm = 147.4 # from the ground including the attached height from the base of the car\n",
    "    real_height_of_the_object_mm = 160 # actual height of the leading car\n",
    "    \n",
    "    return (focal_length_mm * real_height_of_the_object_mm * image_height_pixels) / (object_height_pixels * sensor_height_mm)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Function that calculates the actual real life velocity in kmph based on the optical flow velocity values.\n",
    "#   perspective_angle - perspective angle of camera, for reporting flow in kmph\n",
    "#   move_step - step size in pixels for sampling the flow image\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "def velocity_in_kmph(flow, move_step, sum_velocity_pixels, dimsize_pixels, timestep_seconds, distance_meters, perspective_angle):\n",
    "    count = (flow.shape[0] * flow.shape[1]) / move_step**2\n",
    "    average_velocity_pixels_per_second = sum_velocity_pixels / count / timestep_seconds  \n",
    "    distance_pixels = (dimsize_pixels/2) / math.tan(perspective_angle/2)     \n",
    "    pixels_per_meter = distance_pixels / distance_meters\n",
    "\n",
    "    return average_velocity_pixels_per_second / pixels_per_meter\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Function to draw flow lines from points in the image resulting from the optical flow computation. The flow is represeted as \n",
    "# vectors whose length determines the magnitude and the orientation determines the direction of flow.\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "def draw_flow(img, flow, step=16):\n",
    "    h, w = img.shape[:2]\n",
    "    y, x = np.mgrid[step/2:h:step, step/2:w:step].reshape(2,-1).astype(int)\n",
    "    fx, fy = flow[y,x].T\n",
    "    lines = np.vstack([x, y, x+fx, y+fy]).T.reshape(-1, 2, 2)\n",
    "    lines = np.int32(lines + 0.5)\n",
    "    vis = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.polylines(vis, lines, 0, (0, 255, 0))\n",
    "    for (x1, y1), (_x2, _y2) in lines:\n",
    "        cv2.circle(vis, (x1, y1), 1, (0, 255, 0), -1)\n",
    "        \n",
    "    return vis\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Computes a dense optical flow using the Gunnar Farneback’s algorithm. \n",
    "# Refer to https://github.com/simondlevy/OpenCV-Python-Hacks/blob/master/optical_flow/__init__.py\n",
    "# This function is attached as a callback to the camera frame and the optical flow display widget. Takes a camera frame as input \n",
    "# and returns a processed frame to the widget as output.\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "def optical_flow_function_updated(frame2):\n",
    "    global test_frame_1, prev_time\n",
    "    gray = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    xsum, ysum = 0,0\n",
    "    xvel, yvel = 0,0\n",
    "    move_step = 16\n",
    "    \n",
    "    flow = cv2.calcOpticalFlowFarneback(test_frame_1, gray, None, pyr_scale=0.5, levels=5, winsize=13, iterations=10, poly_n=5, poly_sigma=1.1, flags=0) \n",
    "\n",
    "    frame2 = draw_flow(gray, flow)\n",
    "    \n",
    "    for y in range(0, flow.shape[0], move_step):\n",
    "        for x in range(0, flow.shape[1], move_step):\n",
    "\n",
    "            fx, fy = flow[y, x]\n",
    "#             xsum += fx # Needed if we use velocity along x axis also\n",
    "            ysum += fy\n",
    "    \n",
    "    # Default to system time if no timestep\n",
    "    curr_time = time.time()\n",
    "    timestep = (curr_time - prev_time)\n",
    "    prev_time = curr_time\n",
    "    \n",
    "    # Velocity along the y axis gives us the velocity in the forward direction\n",
    "#     xvel = velocity_in_kmph(flow, move_step, xsum, flow.shape[1], timestep, 3.5, 1)\n",
    "    yvel = velocity_in_kmph(flow, move_step, ysum, flow.shape[0], timestep, 0.35, 1)\n",
    "\n",
    "    test_frame_1 = gray\n",
    "    \n",
    "    im_pil = Image.fromarray(frame2)\n",
    "    draw = ImageDraw.Draw(im_pil)\n",
    "    draw.text((100, 150), 'Velocity: {:.2f}'.format(yvel))\n",
    "    draw.text((90, 280), 'Visual Odometry Window')\n",
    "    del(draw)   \n",
    "    \n",
    "    return bgr8_to_jpeg(np.array(im_pil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# For the Obstacle detection with Depth Perception, we define 6 'windows' that are basically 6 regions in the center of the frame\n",
    "# horizontally adjacent to each other. Based on the intnsity of the grayscale value of the pixels wihin these windows we can sort\n",
    "# of determine which region in the image does the obstace lie.\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# detection_threshold = 0.50\n",
    "# number_of_windows = 6\n",
    "# region = np.empty((6), dtype = object)\n",
    "# obstacle_window = np.empty((6), dtype = object)\n",
    "# averaged_gray_region = np.empty((6), dtype = object)\n",
    "\n",
    "# for i in range(number_of_windows):\n",
    "#     region[i] = (5+(i*50), 130, 50*(i+1), 175) \n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# The perceived depth is color mapped to the gray scale (0-255), wtih 0 being no obstacle and 255 being obstacle in view and being \n",
    "# the closest. Based on the windows defined above we can estimate which area(s) of the image contains the obstacle. If a certain\n",
    "# threshold is crossed by the grayscale value, the 'window' turns red indicating an obstacle. \n",
    "# This function is attached as a callback to the camera frame and the optical flow display widget. Takes a camera frame as input \n",
    "# and returns a processed frame to the widget as output. \n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# def depth_perception(input_image):\n",
    "#     with torch.no_grad():\n",
    "        \n",
    "#         input_image = Image.fromarray(input_image).convert('RGB')\n",
    "#         original_width, original_height = input_image.size\n",
    "#         input_image = input_image.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "#         input_image = transforms.ToTensor()(input_image).unsqueeze(0)\n",
    "\n",
    "#         # PREDICTION\n",
    "#         input_image = input_image.to(device)\n",
    "#         features = encoder(input_image)\n",
    "#         outputs = depth_decoder(features)\n",
    "\n",
    "#         disp = outputs[(\"disp\", 0)]\n",
    "#         disp_resized = torch.nn.functional.interpolate(\n",
    "#             disp, (original_height, original_width), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "#         disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
    "#         vmax = np.percentile(disp_resized_np, 95)\n",
    "#         normalizer = mpl.colors.Normalize(vmin=disp_resized_np.min(), vmax=vmax)\n",
    "#         mapper = cm.ScalarMappable(norm=normalizer, cmap='Greys')\n",
    "#         colormapped_im = (mapper.to_rgba(disp_resized_np)[:, :, :3] * 255).astype(np.uint8)\n",
    "#         im_pil = pil.fromarray(colormapped_im)\n",
    "        \n",
    "#         draw = ImageDraw.Draw(im_pil)\n",
    "        \n",
    "#         # More black equals 'more' obstacle...? :E \n",
    "#         for i in range(number_of_windows):\n",
    "#             # Obstacle detection windows\n",
    "#             obstacle_window[i] = im_pil.crop(region[i]).convert('L')\n",
    "#             averaged_gray_region[i] = np.mean(np.array(obstacle_window[i])/255)\n",
    "            \n",
    "#             if(averaged_gray_region[i] < detection_threshold):\n",
    "#                 draw.rectangle(region[i], outline=\"blue\")\n",
    "#                 draw.text((10+(i*50), 135), '{:.2f}'.format(averaged_gray_region[i]))\n",
    "#             else:\n",
    "#                 draw.rectangle(region[i])\n",
    "#                 draw.text((10+(i*50), 135), '{:.2f}'.format(averaged_gray_region[i]))\n",
    "\n",
    "#         draw.text((90, 280), 'Depth Perception Window')\n",
    "\n",
    "#         del(draw)  \n",
    "\n",
    "#     return bgr8_to_jpeg(np.array(im_pil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Used the Ziegler–Nichols method for the tuning. The Kp, Ki, and Kd values are negative to indicate that the controller must \n",
    "# operate in REVERSE mode with respect to the output limits, as the output limits cannot be defined in descending order.\n",
    "# The set point and the input for the PID is the square root of the central_area and the bounding box area respectively, in an \n",
    "# attempt to convert the area input to a linear value. \n",
    "# Note that these tunings are for the system operating in 10W mode. Tunings will differ slightly for 5W mode.\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "from simple_pid import PID\n",
    "throttle_pid = PID(-0.25, -1.0, -0.25, setpoint = math.sqrt(central_area))\n",
    "throttle_pid.output_limits = (max_throttle_forward, max_throttle_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# This is the main function where the platooning magic happens. The frame is procedded by the object detection model and bounding \n",
    "# boxes for the detected objects (cars) are output. These bounding boxes are passed through the Kalman filter to enable tracking\n",
    "# if more than two cars are part of the platooning system, in order to assign an ID to each of them, thereby letting us decide\n",
    "# which car to follow. Alongside, the area of the bounding box, its center coordinates, the displacement of the bbox centre from \n",
    "# the frame center along the x and y axes, the distance to the tracked object and finally the frames per second are \n",
    "# all calculated respectively. The PID controller then deals with maintaining a relative distance between the leading car and the \n",
    "# following car using the bbox area to throttle mapping. The Steering is interesting though since first the horizontal \n",
    "# displacement value (along the x axis) is observed, and the pan and the tilt functionalities are tasked with alligning the \n",
    "# camera itself to the center of the object. Following this the tyres are actuated to orient itself to the centroid \n",
    "# of the bounding box. Simply put, the camera follows the tracked object and the steering then follows the centre of the bbox \n",
    "# in the frame, along the x axis. This function is attached as a callback to the camera frame and the object detection \n",
    "# output display widget. Takes a camera frame as input and returns a processed frame to the widget as output.\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def object_detection_and_following(image):\n",
    "    global last_known_position, lost_timer, last_known_pan, last_known_tilt\n",
    "    \n",
    "    image = np.array(Image.fromarray(image))\n",
    "    im_pil = Image.fromarray(image)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    scores, boxes, classes, num_detections = tf_sess.run([tf_scores, tf_boxes, tf_classes, tf_num_detections], feed_dict={tf_input: image[None, ...]})\n",
    "    t1 = time.time()\n",
    "    boxes = boxes[0]\n",
    "    scores = scores[0]\n",
    "    classes = classes[0]\n",
    "    num_detections = num_detections[0]\n",
    "       \n",
    "    if(num_detections > 0 and classes == 1):\n",
    "    \n",
    "        # inserting a kalman filter to track the particular object\n",
    "        kf_boxes = kalman_object_tracker.update(boxes)\n",
    "        \n",
    "        # for some reason kalman doesnt track at times. Has to do with the new mean being completely different from the previous frames (mean)\n",
    "        if(kf_boxes.any()):\n",
    "            box = kf_boxes[0][:4] * np.array([image.shape[0], image.shape[1], image.shape[0], image.shape[1]]) \n",
    "            track_status = 'tracking'\n",
    "        else:\n",
    "            box = boxes[0] * np.array([image.shape[0], image.shape[1], image.shape[0], image.shape[1]])\n",
    "            track_status = 'lost'\n",
    "\n",
    "        bb_height = box[3] - box[1]\n",
    "        bb_width = box[2] - box[0]\n",
    "\n",
    "        centre_x = (box[3] + box[1])/2\n",
    "        centre_y = (box[2] + box[0])/2\n",
    "        \n",
    "        area = bb_width * bb_height\n",
    "        \n",
    "        fps = 1.0 / (t1 - t0)\n",
    "        \n",
    "        h_obj_position = centre_x - 150\n",
    "        v_obj_position = centre_y - 150\n",
    "        \n",
    "        n_obj_distance = distance_to_object(bb_width)\n",
    "        \n",
    "        draw = ImageDraw.Draw(im_pil)\n",
    "        draw.rectangle(((box[3], box[2]), (box[1], box[0])))\n",
    "        draw.line([(150, 0), (150, 300)]) # vertical screen splitter\n",
    "        draw.line([(0, 150), (300, 150)]) # horizontal screen splitter\n",
    "        draw.point((centre_x, centre_y))\n",
    "        draw.text((10, 5), 'area = {:.2f}'.format(area)) # bounding box area\n",
    "        draw.text((10, 20), 'fps = {:.2f}'.format(fps)) # frame rate\n",
    "        draw.text((10, 35), 'h_position = {:.2f}'.format(h_obj_position)) # horizontal position\n",
    "        draw.text((10, 50), 'v_position = {:.2f}'.format(v_obj_position)) # vertical position\n",
    "        draw.text((10, 65), 'distance = {:.2f}'.format(n_obj_distance)) # distance to the leading car\n",
    "        draw.text((10, 80), 'k_tracking = {}'.format(track_status)) # kalman filter tracking status\n",
    "        draw.text((10, 95), 'probability = {}'.format(scores)) # confidence of the detection\n",
    "        draw.text((10, 110), 'class = {}'.format(classes))\n",
    "        draw.text((90, 280), 'Object Detection Window')\n",
    "        del(draw)  \n",
    "        \n",
    "        # run the PID controller for throttle vs area, with sqrt(area) as input. Hats off to anyone who manages to tune it well. I give up :/\n",
    "        track_throttle = throttle_pid(math.sqrt(area))\n",
    "        \n",
    "        track_steering = mapper(h_obj_position, -150, 150, 135, 45)\n",
    "        track_steering = constrain(track_steering, 45, 135)         \n",
    "               \n",
    "        track_pan = constrain(mapper(h_obj_position, -150, 150, 120, 60), 60, 120)\n",
    "        track_tilt = constrain(mapper(v_obj_position, -150, 150, 0, 40), 0, 40)\n",
    "                \n",
    "        serial_writer(int(track_steering), int(track_throttle), int(track_pan), int(track_tilt))\n",
    "        last_known_position = track_steering\n",
    "        last_known_pan = track_pan\n",
    "        last_known_tilt = track_tilt\n",
    "        lost_timer = 0\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------------------------------------------------#\n",
    "    # If there are no detections or the object being detected was/is lost, the last know steering angle is stored and recalled, and the \n",
    "    # following car maintains this steering angle with a crawling throttle hoping that it would encounter the lost object to be \n",
    "    # followed. A 'timer' also runs alongside and upon expiry bring the following car to a halt. Could be improved for better lost \n",
    "    # tracking object management.\n",
    "    #----------------------------------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "    if(num_detections == 0 and last_known_position != 555):\n",
    "        lost_timer = lost_timer + 1\n",
    "        \n",
    "        draw = ImageDraw.Draw(im_pil)\n",
    "        draw.text((10, 5), 'last known position = {}'.format(last_known_position))\n",
    "        draw.text((10, 20), 'lost counter = {}'.format(lost_timer))\n",
    "        draw.text((10, 150), 'Reacquiring lost tracking object.')\n",
    "        del(draw)\n",
    "        \n",
    "        serial_writer(int(last_known_position), int(crawling_throttle), int(last_known_pan), int(last_known_tilt))        \n",
    "        if(lost_timer > 20):\n",
    "            serial_writer(int(zero_steering), int(zero_throttle), int(zero_pan), int(zero_tilt)) \n",
    "            last_known_position = 555\n",
    "            last_known_pan = 555\n",
    "                \n",
    "    if(num_detections == 0 and last_known_position == 555):\n",
    "        \n",
    "        draw = ImageDraw.Draw(im_pil)\n",
    "        draw.text((10, 150), 'Tracking object lost or uninitialized.')\n",
    "        del(draw)\n",
    "        \n",
    "        serial_writer(int(zero_steering), int(zero_throttle), int(zero_pan), int(zero_tilt))\n",
    "        \n",
    "    return bgr8_to_jpeg(np.array(im_pil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure camera is initialized, run it twice maybe till you get the output of the camera as a display.\n",
    "camera.running = True\n",
    "plt.imshow(camera.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# The Traitlets act as threaded callback linkers between the updating camera frames and the functions that utilize the camera \n",
    "# frames. This is included as a 'transformation function' in the Traitlet function call and definition. Based on whether you'd\n",
    "# like just platooning, or platooning + optical flow velocity estimation or platooning + optical flow velocity estimation \n",
    "# + obstacle detection with depth perception, uncomment the trailtets accordingly. Same goes for the widget displays.\n",
    "# A warning, using more than just platooning slows down processing considerably.   \n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "try:\n",
    "    camera.running = True\n",
    "    camera1_link = traitlets.dlink((camera, 'value'), (output_image_widget, 'value'), transform = object_detection_and_following)\n",
    "#     camera4_link = traitlets.dlink((camera, 'value'), (optical_flow_vector_image_widget, 'value'), transform = optical_flow_function_updated)\n",
    "#     camera5_link = traitlets.dlink((camera, 'value'), (depth_perception_widget, 'value'), transform = depth_perception)\n",
    "    \n",
    "    camera1_link.link()\n",
    "#     camera4_link.link()\n",
    "#     camera5_link.link()\n",
    "    \n",
    "except Exception as ex: \n",
    "    camera.running = False   \n",
    "    print(ex)    \n",
    "    \n",
    "    camera1_link.unlink()\n",
    "\n",
    "#     camera4_link.unlink()\n",
    "#     camera5_link.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(HBox([output_image_widget, optical_flow_vector_image_widget, depth_perception_widget]))\n",
    "display(HBox([output_image_widget]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "# To make any kind of changes to the detection function run this first. \n",
    "# Camera has to be stopped first or else everything else goes kaput.\n",
    "#----------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "camera.running = False\n",
    "camera1_link.unlink()\n",
    "# camera4_link.unlink()\n",
    "# camera5_link.unlink()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
